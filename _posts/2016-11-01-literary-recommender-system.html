---
title: "Building a Content-Based Recommender System for Books: Using Natural Language Processing to Understand Literary Preference"
categories: projects ML data science
excerpt: Literature is a tricky area for data science. Think of your five favorite books. What do they have in common? Some may share an author or genre, but besides that, it is probably hard for you to think of what traits they share. My team and I set out to explore the mysterious components of an individual’s literary taste profile, and in the process built a content-based recommender system for books. This post is a brief overview of the system, the features it uses, and how it was built.
---
<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Designing a Content-Based Recommender System for Books</title><meta name="description" content="Using Natural Language Processing to Understand Literary Preference"><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 740px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Designing a Content-Based Recommender System for Books</h1>
</header>
<section data-field="subtitle" class="p-summary">
Using Natural Language Processing to Understand Literary Preference
</section>
<section data-field="body" class="e-content">
<section name="9d4a" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="97fc" id="97fc" class="graf graf--h3 graf--leading graf--title">Designing a Content-Based Recommender System for Books</h3><p name="e6e3" id="e6e3" class="graf graf--p graf-after--h4"><strong class="markup--strong markup--p-strong">Literature is a tricky area for data science.</strong> Think of your five favorite books. What do they have in common? Some may share an author or genre, but besides that, it is probably hard for you to think of what traits they share. <a href="https://www.linkedin.com/in/drewhoo" data-href="https://www.linkedin.com/in/drewhoo" class="markup--anchor markup--p-anchor" target="_blank">Drew Hoo</a>, <a href="https://www.linkedin.com/in/aniket-saoji-b7b40955" data-href="https://www.linkedin.com/in/aniket-saoji-b7b40955" class="markup--anchor markup--p-anchor" target="_blank">Aniket Saoji</a> and I set out to explore the mysterious components of an individual’s literary taste profile, and in the process built a content-based recommender system for books. In this post I will give a brief overview of the system, the features it uses, and how it was built. For a deeper dive into the nuts and bolts of the system, see the github repo <a href="https://github.com/jldbc/gutenberg" data-href="https://github.com/jldbc/gutenberg" class="markup--anchor markup--p-anchor" target="_blank">here</a>.</p><h3 name="a0af" id="a0af" class="graf graf--h3 graf-after--p"><strong class="markup--strong markup--h3-strong">Data</strong></h3><p name="b517" id="b517" class="graf graf--p graf-after--h3">The first challenge was finding data. People’s literary tastes span an incredibly wide range of topics and styles; to effectively model these we need a large amount of data on each of these categories. For this reason, our approach was to get as much data as possible. We pulled the text file for every book, magazine, almanac, and so-on from the world’s largest Internet library, <a href="http://www.gutenberg.org/" data-href="http://www.gutenberg.org/" class="markup--anchor markup--p-anchor" target="_blank">Project Gutenberg</a>. The data set totaled 55,000 texts, which amounted to approximately 22gb of raw text. </p><h3 name="22f0" id="22f0" class="graf graf--h3 graf-after--p"><strong class="markup--strong markup--h3-strong">Feature Extraction</strong></h3><p name="0834" id="0834" class="graf graf--p graf-after--h3">With the data obtained, we then had to do something with it. Raw text is unstructured data, so in order to perform any useful analyses or recommendations, we first have to extract features from the text. To begin with the low-hanging fruit, we first extract the title and author of the text, since these are conveniently placed in the header of each Project Gutenberg text file with consistent, easy to parse formatting. From here, however, it gets more interesting. </p><h4 name="d94f" id="d94f" class="graf graf--h4 graf-after--p">Punctuation and Part-of-Speech Profiles</h4><p name="8f7e" id="8f7e" class="graf graf--p graf-after--h4">We were next inspired by <a href="https://medium.com/@neuroecology/punctuation-in-novels-8f316d542ec4#.mf4s5ru6g" data-href="https://medium.com/@neuroecology/punctuation-in-novels-8f316d542ec4#.mf4s5ru6g" class="markup--anchor markup--p-anchor" target="_blank">this post</a> from <a href="https://medium.com/u/f96422ab6f6b" data-href="https://medium.com/u/f96422ab6f6b" data-anchor-type="2" data-user-id="f96422ab6f6b" data-action-value="f96422ab6f6b" data-action="show-user-card" data-action-type="hover" class="markup--user markup--p-user" target="_blank">Adam J Calhoun</a>, visualizing different authors’ punctuation profiles. A punctuation profile includes the relative frequency of each punctuation mark’s use, compared to the text’s punctuation count as a whole. Extracting this profile from each text, we found common patterns that proved useful in understanding links between texts. Theatrical texts, for example, had high scores for semicolon use, due to their common structure of beginning each line with </p><blockquote name="4fc5" id="4fc5" class="graf graf--blockquote graf-after--p">CHARACTER NAME: (the character’s line)</blockquote><p name="033c" id="033c" class="graf graf--p graf-after--blockquote">Similarly, poetry scored particularly low for period usage, and mathematical texts scored high for exclamation point and plus sign usage, due to their usage in mathematical notation. These results show that punctuation profiles help to understand not only literary style, but also basic structural and categorical features of a text. An example of punctuation profiles, from Calhoun’s analysis, is shown below.</p><figure name="19dd" id="19dd" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 488px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 69.8%;"></div><img class="graf-image" data-image-id="1*l92tMZLtNxUw9v3HuFXjYQ.png" data-width="1128" data-height="787" src="https://cdn-images-1.medium.com/max/1600/1*l92tMZLtNxUw9v3HuFXjYQ.png"></div></figure><p name="b5fe" id="b5fe" class="graf graf--p graf-after--figure">Moving past punctuation profiles, we then explored part-of-speech profiles, examining authors’ relative use of verbs, adjectives, pronouns, and other parts of speech. These results were similarly useful in identifying patterns between works. A few patterns we found were that novels tended to rely heavily on verbs in order to share plot points, where scientific texts tended to be rely heavily in adjective use in order to describe their discoveries. </p><h4 name="c5ff" id="c5ff" class="graf graf--h4 graf-after--p">Sentiment Analysis</h4><p name="59e7" id="59e7" class="graf graf--p graf-after--h4">Our next set of features were sentiment scores. An important feature in someone’s literary tastes is whether they enjoy books that are generally positive, negative, or neutral in sentiment. We used Natural Language Toolkit’s <a href="https://github.com/cjhutto/vaderSentiment" data-href="https://github.com/cjhutto/vaderSentiment" class="markup--anchor markup--p-anchor" target="_blank">VADER sentiment analyzer</a> in order to extract this information. VADER proved preferable to the more commonly used Naive Bayes approach because it takes lexical and rule-based patterns into account such as capitalization and punctuation patterns, which help to catch difficult-to-identify features such as sarcasm and emotion. “STOP!,” for example, has stronger emotional value attached than “stop.,” and this type of difference is captured particularly well by VADER. </p><p name="d9e7" id="d9e7" class="graf graf--p graf-after--p">In general there was very little variance between texts in terms of their positive and negative sentiment scores. The most useful result of using sentiment analysis, contrary to expectations, was that it did an excellent job of identifying academic texts. The highest scoring texts in the “neutral” sentiment feature included Darwin’s <em class="markup--em markup--p-em">Coral Reefs, </em>various biology and chemistry textbooks, history books, and various obscure almanacs and fact books.</p><h4 name="28b8" id="28b8" class="graf graf--h4 graf-after--p">Content Clustering</h4><p name="136e" id="136e" class="graf graf--p graf-after--h4">The final and most important feature we extracted was each book’s content classification. The general strategy for obtaining this was to use three layers of clustering: one high-level cluster, for capturing genre-level similarity, one intermediate-level cluster, for capturing the topic of the text, and one highly specific cluster, for capturing niche categories and sub-categories of content. We do this using k-means clustering on each book’s TF-IDF scores, using three different values for the number of clusters.</p><p name="5b30" id="5b30" class="graf graf--p graf-after--p">A <strong class="markup--strong markup--p-strong">TF-IDF score</strong> is a word’s term frequency times its inverse document frequency. In short, this represents how important a word is to the book it belongs to, relative to how common it is in the broader world of literature. The insight of this score is that if a word is used heavily in one text, and rarely in the rest of literature, then that word is a distinguising feature for that particular text. Calculating these scores for all 55,000 of Project Gutenberg’s e-books was computationally challenging, but made possible by Apache Spark and Amazon Web Services’ Elastic Mapreduce platform. </p><p name="9c0b" id="9c0b" class="graf graf--p graf-after--p">Using these TF-IDF scores, we then used k-means to identify clusters of similar books. K-means is a simple clustering algorithm that assigns data points to clusters based on their proximity to a cluster centroid, which is a point at the center of the cluster. The algorithm iteratively updates centroid locations and re-assigns data points to their new clusters until the cluster memberships stabilize, thus finding the best clustering results for that set of centroids. With each book’s array of TF-IDF scores as our data points, we ran k-means clustering for a small, medium, and large number of clusters, capturing general, intermediate, and granular degrees of similarity between the books’ vocabularies. </p><figure name="ae7c" id="ae7c" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 561px; max-height: 420px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 74.9%;"></div><img class="graf-image" data-image-id="1*KV7q9k9Kf44djmU-0svQqQ.jpeg" data-width="561" data-height="420" src="https://cdn-images-1.medium.com/max/1600/1*KV7q9k9Kf44djmU-0svQqQ.jpeg"></div><figcaption class="imageCaption">K-Means visualization in 2 Dimensions</figcaption></figure><p name="e204" id="e204" class="graf graf--p graf-after--figure">The clustering results were by far the most useful for making recommendations, accurately identifying books that were similar in content. Some examples of what the three levels of clustering found are shown in the tables below. </p><figure name="3909" id="3909" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 369px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 52.7%;"></div><img class="graf-image" data-image-id="1*vbVtJcx2Y5G4frFW__TecA.png" data-width="1264" data-height="666" src="https://cdn-images-1.medium.com/max/1600/1*vbVtJcx2Y5G4frFW__TecA.png"></div><figcaption class="imageCaption">General Theme Clusters: K = 50</figcaption></figure><figure name="de7f" id="de7f" class="graf graf--figure graf-after--figure"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 270px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 38.5%;"></div><img class="graf-image" data-image-id="1*o-LPIqkeXA1Lb8xFUa0b3A.png" data-width="1266" data-height="488" src="https://cdn-images-1.medium.com/max/1600/1*o-LPIqkeXA1Lb8xFUa0b3A.png"></div><figcaption class="imageCaption">Intermediate-Level Clusters: K = 155</figcaption></figure><figure name="a8ad" id="a8ad" class="graf graf--figure graf-after--figure"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 352px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 50.2%;"></div><img class="graf-image" data-image-id="1*Fah8t5cOPqLrrhemdnuc1w.png" data-width="1262" data-height="634" src="https://cdn-images-1.medium.com/max/1600/1*Fah8t5cOPqLrrhemdnuc1w.png"></div><figcaption class="imageCaption">Niche Content Clusters: K = 480</figcaption></figure><p name="7f84" id="7f84" class="graf graf--p graf-after--figure">Additional to these features, we also extracted average sentence length and average word length for a basic understanding of the text’s length and the complexity of its vocabulary.</p><h3 name="978a" id="978a" class="graf graf--h3 graf-after--p"><strong class="markup--strong markup--h3-strong">Forming Recommendations</strong></h3><p name="2ac1" id="2ac1" class="graf graf--p graf-after--h3">To recap the features extracted thus far, our data now contains each book’s punctuation profile, part-of-speech profile, positive, negative, and neutral sentiment score, author, average sentence length, average word length, and three degrees of content classification. With this in place, we are now ready to implement recommendation algorithms. </p><p name="43cd" id="43cd" class="graf graf--p graf-after--p">Forming recommendations is quite simple if the data is well-structured. After some experimentation with feature selection, we recommended texts using a k-nearest neighbors algorithm. This is among the most straightforward machine learning algorithms, while simultaneously one of the most reliable and effective for recommendation tasks. Given the input of a book that a user has enjoyed, the algorithm searches through our available book data and finds the ones that are most similar according to the features we have extracted. Using this algorithm, we are able to take one or more books as a baseline for a user’s literary taste profile and recommend any number of similar texts based on the hundreds of features we have extracted from each book. </p><h3 name="65de" id="65de" class="graf graf--h3 graf-after--p"><strong class="markup--strong markup--h3-strong">Final Results</strong></h3><p name="01dd" id="01dd" class="graf graf--p graf-after--h3">The final results of our system are shown below. Potential next steps for improving the system include obtaining user ratings data, which would allow for more precise and personalized recommendations, and finding a way to obtain more modern titles for the recommender system, as Project Gutenberg does not contain books that are still controlled by their publishing houses. In the end, the results were encouraging, with the system’s recommendations making strong intuitive sense. </p><figure name="d012" id="d012" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 695px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 99.3%;"></div><img class="graf-image" data-image-id="1*j27Gll1EsgvADuZDH9V4EA.png" data-width="1128" data-height="1120" src="https://cdn-images-1.medium.com/max/1600/1*j27Gll1EsgvADuZDH9V4EA.png"></div></figure><p name="6f9b" id="6f9b" class="graf graf--p graf-after--figure graf--last">The code for this project can be found on its github repo <a href="https://github.com/jldbc/gutenberg" data-href="https://github.com/jldbc/gutenberg" class="markup--anchor markup--p-anchor" target="_blank">here</a>. </p></div></div></section>
</section>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-52953508-1', 'auto');
  ga('send', 'pageview');

</script>
<footer><p><a href="https://medium.com/p/8a7a08c4cbe8">View original.</a></p><p>Exported from <a href="https://medium.com">Medium</a> on December 22, 2016.</p></footer></article>

</body></html>